{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUt78HBJhIvE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer, pipeline, logging\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and Dataset Configurations"
      ],
      "metadata": {
        "id": "Qz0JKUu2h-Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "num_workers = os.cpu_count()\n",
        "max_steps = 3000\n",
        "bf16 = False\n",
        "fp16 = True\n",
        "gradient_accumulation_steps = 2\n",
        "learning_rate = 0.0001\n",
        "context_length = 256\n",
        "logging_steps = 500\n",
        "save_steps = 500\n",
        "model_name = \"openai-community/gpt2\"\n",
        "out_dir = \"outputs/gpt_alpaca_preprocess_fn\""
      ],
      "metadata": {
        "id": "bUrIYr_Mh8PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the Alpaca Instruction Tuning Dataset"
      ],
      "metadata": {
        "id": "r3x86dqGjEnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "nA7Bx1MyjDSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = dataset['train'].train_test_split(test_size=0.5, shuffle=True)\n",
        "dataset_train = full_dataset['train']\n",
        "dataset_valid = full_dataset['test']\n",
        "\n",
        "print(dataset_train)\n",
        "print(dataset_valid)"
      ],
      "metadata": {
        "id": "jBmAo4FxjAIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "MUMonZkjlhxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the GPT2 Base Model for Instruction Tuning"
      ],
      "metadata": {
        "id": "CmOXOBhOmq2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if bf16:\n",
        "  model = AutoMdelForCausalLM.from_pretrained(model_name).to(dtype=torch.bfloat16)\n",
        "else:\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "oYjVupbsl60K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")"
      ],
      "metadata": {
        "id": "rXxhtVt7mRnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing the Tokenizer"
      ],
      "metadata": {
        "id": "LcK4gACKmk2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "J9WxS4UVmebi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the GPT2 Model on the Alpaca Dataset"
      ],
      "metadata": {
        "id": "rp0dN0Usm89o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{out_dir}/logs\",\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    evaluation_strategy='steps',\n",
        "    logging_strategy='steps',\n",
        "    save_strategy='steps',\n",
        "    logging_steps=logging_steps,\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=2,\n",
        "    bf16=bf16,\n",
        "    fp16=fp16,\n",
        "    report_to='tensorboard',\n",
        "    max_steps=max_steps,\n",
        "    dataloader_num_workers=num_workers,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type='constant',\n",
        ")"
      ],
      "metadata": {
        "id": "ep3gpha6m_jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_valid,\n",
        "    max_seq_length=context_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    formating_function=preprocess_function,\n",
        "    packing=True,\n",
        ")"
      ],
      "metadata": {
        "id": "3FPA7rASoCYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = trainer.get_train_dataloader()\n",
        "\n",
        "for i, sample in enumerate(dataloader):\n",
        "    print(tokenizer.decode(sample['input_ids'][0]))\n",
        "    print('#'*50)\n",
        "    if i == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "zI7AqrNQolE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = trainer.train()"
      ],
      "metadata": {
        "id": "nhN-DNELo9v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f\"{out_dir}/model\")\n",
        "tokenizer.save_pretrained(f\"{out_dir}/tokenizer\")"
      ],
      "metadata": {
        "id": "_oXjwGzzpH3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference"
      ],
      "metadata": {
        "id": "BtpiY2BBpP7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('outputs/gpt2_alpaca_preprocess_fn/best_model/')\n",
        "tokenizer = AutoTokenizer.from_pretrained('outputs/gpt2_alpaca_preprocess_fn/best_model/')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "I49u6XDZpS5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "DzGaKsnGpiNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"### Instruction:\n",
        "{}\n",
        "### Input:\n",
        "{}\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "pmV0AYQVpuN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = 'Write three tips for staying healthy.'\n",
        "inputs = ''\n",
        "response = ''\n",
        "prompt = template.format(instructions, inputs, response)"
      ],
      "metadata": {
        "id": "8mPCOjqGpxD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "QeiDs6mrp1vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}